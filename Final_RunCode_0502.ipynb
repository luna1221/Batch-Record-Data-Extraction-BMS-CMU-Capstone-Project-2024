{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyPDF2\n",
    "pip install Pillow\n",
    "pip install numpy\n",
    "pip install pandas\n",
    "pip install PyMuPDF\n",
    "pip install pytesseract\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe' #adjust as needed based on path: for windows\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/local/bin/tesseract' # for mac\n",
    "pip install IPython\n",
    "pip install fitz\n",
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import PyPDF2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import fitz \n",
    "import pytesseract\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Choosing Merged PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the merged PDF from your folder\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "# Create a Tkinter root window\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "\n",
    "# Ask the user to select a file\n",
    "pdf_path = filedialog.askopenfilename(title=\"Select PDF file\")\n",
    "\n",
    "# Check if the user selected a file\n",
    "if pdf_path:\n",
    "    print(\"Selected PDF file:\", pdf_path)\n",
    "else:\n",
    "    print(\"No file selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the directory path from the pdf_path\n",
    "pdf_directory = os.path.dirname(pdf_path)\n",
    "\n",
    "# Define the paths for the new folders\n",
    "pdf_folder = os.path.join(pdf_directory, \"pdf_folder\")\n",
    "\n",
    "# Create the new folders\n",
    "if not os.path.exists(pdf_folder):\n",
    "    os.makedirs(pdf_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Splitting the individual PDFs from the Merged PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_firstpage(page_text):\n",
    "    # Regular expression pattern to find \"Page: 1 of X\" footer text\n",
    "    pattern = r\"Page:\\s*1\\s*of\\s*(\\d+)\"\n",
    "    match = re.search(pattern, page_text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        total_pages = len(reader.pages)\n",
    "        current_page = 0\n",
    "        section_counter = 1\n",
    "        \n",
    "        # Create the pdf folder if it doesn't exist\n",
    "        if not os.path.exists(pdf_folder):\n",
    "            os.makedirs(pdf_folder)\n",
    "        \n",
    "        while current_page < total_pages:\n",
    "            writer = PyPDF2.PdfWriter()\n",
    "            start_page = current_page + 1\n",
    "            \n",
    "            # Extract text from the first page of the section\n",
    "            first_page = reader.pages[current_page]\n",
    "            page_text = first_page.extract_text()\n",
    "            footer_text = find_firstpage(page_text)\n",
    "            \n",
    "            if footer_text:\n",
    "                # If footer text is found, split the section based on the number of pages indicated\n",
    "                for _ in range(footer_text):\n",
    "                    if current_page < total_pages:\n",
    "                        writer.add_page(reader.pages[current_page])\n",
    "                        current_page += 1\n",
    "            else:\n",
    "                # If no footer text is found, treat the section as a single-page section\n",
    "                writer.add_page(first_page)\n",
    "                current_page += 1\n",
    "                \n",
    "            pdf_name = f\"pdf{section_counter}.pdf\"\n",
    "            pdf_pdf_path = os.path.join(pdf_folder, pdf_name)\n",
    "            #pdf_pdf_path = os.path.join(pdf_folder, pdf_path.replace('.pdf', f'_part{start_page}_to_{current_page}.pdf').split('/')[-1])\n",
    "            with open(pdf_pdf_path, 'wb') as out:\n",
    "                writer.write(out)\n",
    "                \n",
    "            section_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Rotating pages for each PDF in the correct orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_orientation(page):\n",
    "    # Extract text elements\n",
    "    text_elements = page.get_text(\"dict\")[\"blocks\"]\n",
    "    \n",
    "    # Analyze text block distribution\n",
    "    num_vertical_blocks = 0\n",
    "    num_horizontal_blocks = 0\n",
    "    for block in text_elements:\n",
    "        bbox = block[\"bbox\"]\n",
    "        width = bbox[2] - bbox[0]\n",
    "        height = bbox[3] - bbox[1]\n",
    "        if width > height:\n",
    "            num_horizontal_blocks += 1\n",
    "        else:\n",
    "            num_vertical_blocks += 1\n",
    "    \n",
    "    # Determine orientation based on block distribution\n",
    "    if num_horizontal_blocks > num_vertical_blocks:\n",
    "        return \"Portrait\"\n",
    "    else:\n",
    "        return \"Landscape\"\n",
    "\n",
    "def rotate_page_to_portrait(pdf_path):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    rotated_pages = []\n",
    "\n",
    "    # Iterate through each page\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        \n",
    "        # Get the page orientation\n",
    "        orientation = get_page_orientation(page)\n",
    "        \n",
    "        # Check if the page is not in portrait orientation\n",
    "        if orientation == \"Portrait\":\n",
    "            # Rotate the page to portrait (90 degrees)\n",
    "            page.set_rotation(0)\n",
    "        elif orientation == \"Landscape\":\n",
    "            # Rotate the page to portrait (0 degrees)\n",
    "            page.set_rotation(90)\n",
    "            rotated_pages.append(page_number + 1)  # Store the page number\n",
    "            \n",
    "    # Save the modified PDF with rotated pages to a temporary file\n",
    "    temp_pdf_path = os.path.splitext(pdf_path)[0] + \"_temp.pdf\"\n",
    "    pdf_document.save(temp_pdf_path)\n",
    "\n",
    "    # Close the PDF document\n",
    "    pdf_document.close()\n",
    "\n",
    "    # Replace the original PDF with the rotated one\n",
    "    os.replace(temp_pdf_path, pdf_path)\n",
    "\n",
    "    return rotated_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rotate all PDFs to the correct orientation \n",
    "for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):  # Check if the file is a PDF\n",
    "            pdf_file = os.path.join(pdf_folder, filename)\n",
    "            rotate_page_to_portrait(pdf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Deleting pages in PDF except the page containing Composition Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_in_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        composition_found = False\n",
    "\n",
    "        composition_word = 'composition'\n",
    "        composition_sublot = 'composition sublot a and b'\n",
    "        formulation_word = 'formulation'\n",
    "        composition_solvent = 'solvent composition'\n",
    "\n",
    "        # Loop through each page to find \"composition\" or \"formulation\"\n",
    "        for page_num in range(num_pages):\n",
    "            page = reader.pages[page_num]\n",
    "            text = page.extract_text().lower()\n",
    "            \n",
    "            # Check if \"composition\" is present\n",
    "            if composition_word in text:\n",
    "                composition_found = True\n",
    "                if composition_solvent in text:\n",
    "                    composition_count = text.count(composition_word)\n",
    "                    if composition_count > 1:\n",
    "                        return page_num + 1\n",
    "                elif composition_sublot in text:\n",
    "                    continue\n",
    "                else:\n",
    "                    return page_num + 1\n",
    "                    \n",
    "        if not composition_found:\n",
    "            for page_num in range(num_pages):\n",
    "                page = reader.pages[page_num]\n",
    "                text = page.extract_text().lower()\n",
    "\n",
    "                if formulation_word in text:\n",
    "                    return page_num + 1\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pages(pdf_path, page_to_keep):\n",
    "    temp_path = 'temp.pdf'  # Temporary file path\n",
    "\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        writer = PyPDF2.PdfWriter()\n",
    "\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            if page_num + 1 == page_to_keep:\n",
    "                page = reader.pages[page_num]\n",
    "                writer.add_page(page)\n",
    "\n",
    "        with open(temp_path, 'wb') as output_file:\n",
    "            writer.write(output_file)\n",
    "\n",
    "    # Replace the original file with the temporary file\n",
    "    os.replace(temp_path, pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each PDF file in the pdf_folder\n",
    "def extract_index(filename):\n",
    "    # Use regular expression to find the numerical part of the filename\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        return -1 \n",
    "\n",
    "# Iterate through each PDF file in the pdf_folder\n",
    "for filename in sorted(os.listdir(pdf_folder), key=extract_index):\n",
    "    if filename.endswith(\".pdf\"): \n",
    "        pdf_file = os.path.join(pdf_folder, filename)\n",
    "        \n",
    "        page_number = find_word_in_pdf(pdf_file)\n",
    "        print(f\"Filename: {filename}, Page number: {page_number}\")\n",
    "        \n",
    "        if page_number is not None:\n",
    "            delete_pages(pdf_file, page_number)\n",
    "        \n",
    "        elif page_number is None:\n",
    "            os.remove(pdf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Extracting Batch Number + Composition Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_batchnumber(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    # Use regular expression to find \"batch number:\" followed by any characters (.*)\n",
    "    match = re.search(r'number:\\s*(.*)', text, re.IGNORECASE)\n",
    "\n",
    "    if match:\n",
    "        batch_number = match.group(1).strip()\n",
    "        # Capitalize the words in the batch number\n",
    "        batch_number = batch_number.upper()\n",
    "        return batch_number\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from img2table.document import PDF\n",
    "from img2table.ocr import TesseractOCR\n",
    "from IPython.display import display_html\n",
    "\n",
    "# Set environment path in the script\n",
    "os.environ['PATH'] += os.pathsep + '/opt/homebrew/bin' \n",
    "# set correct path #/opt/homebrew/bin is typical for Unix-like systems (macOS and Linux) where Homebrew, a package manager, is installed\n",
    "\n",
    "# Windows-style paths: For example, if you've installed a program (like Tesseract OCR) in the Program Files directory, you would update your PATH like this\n",
    "# os.environ['PATH'] += os.pathsep + 'C:\\\\Program Files\\\\Tesseract-OCR'\n",
    "\n",
    "# Instantiate OCR\n",
    "ocr = TesseractOCR(n_threads=1, lang=\"eng\")\n",
    "\n",
    "def extract_pdfcompositiontable(pdf_path): #def extract_pdfcompositiontable(pdf_path):\n",
    "    pdf = PDF(pdf_path)\n",
    "    local_dfs = [] \n",
    "\n",
    "    # Extract tables using the correct function\n",
    "    extracted_tables = pdf.extract_tables(ocr=ocr)\n",
    "\n",
    "    # Get all tables from the single page (assuming page 0)\n",
    "    extracted_tables = extracted_tables.get(0, [])  # Get tables from page 0 (default empty list if not found)\n",
    "    \n",
    "    for table in extracted_tables[1:]:\n",
    "        df = table.df\n",
    "        \n",
    "        # Check if all column headers are integers\n",
    "        if all(isinstance(col, int) for col in df.columns):\n",
    "            # Drop the existing numeric header and set the first row as the new header\n",
    "            df.columns = df.iloc[0]\n",
    "            df = df[1:].reset_index(drop=True)\n",
    "                \n",
    "        # Replace \"\\n\" in df\n",
    "        df.columns = df.columns.str.replace(\"\\n\", \" \")\n",
    "        df = df.replace(\"\\n\", \" \", regex=True)\n",
    "        \n",
    "        # Append the extracted DataFrame to the local list\n",
    "        local_dfs.append(df)\n",
    "    \n",
    "    return local_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Save list of dfs to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Function to extract the numerical part of the filename\n",
    "def extract_index(filename):\n",
    "    # Use regular expression to find the numerical part of the filename\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# List to store DataFrames extracted from each pdf\n",
    "dfs = []\n",
    "\n",
    "# Sort pdf filenames based on their numerical part\n",
    "pdf_filenames = sorted([filename for filename in os.listdir(pdf_folder) if filename.endswith(\".pdf\")], key=extract_index)\n",
    "\n",
    "for pdf_filename in pdf_filenames:\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_filename)\n",
    "\n",
    "    # Extract tables from the pdf\n",
    "    tables = extract_pdfcompositiontable(pdf_path)\n",
    "    batch_number = extract_batchnumber(pdf_path)\n",
    "\n",
    "    if tables:\n",
    "        for table in tables:\n",
    "\n",
    "            # Insert the batch number as the first column of the DataFrame\n",
    "            table.insert(0, 'Batch Number', batch_number)\n",
    "            \n",
    "            # Append the table to the list of DataFrames\n",
    "            dfs.append(table.iloc[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dfs as text file for spaCy\n",
    "def dfs_to_text(df_list, file_path):\n",
    "    result_list = []\n",
    "    for df in df_list:\n",
    "        # Drop NaN columns\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "        # Convert DataFrame to list of dictionaries\n",
    "        data = [{\"text\": \" \".join(str(val) for val in row.values)} for _, row in df.iterrows()]\n",
    "        # Append data to result list\n",
    "        result_list.extend(data)\n",
    "        \n",
    "        \n",
    "    result_list = [result_list]\n",
    "\n",
    "    # Write text data to file\n",
    "    with open(file_path, \"w\") as outfile:\n",
    "        for idx, item in enumerate(result_list):\n",
    "            # Create a dictionary with \"text\" as key and the row as value\n",
    "            outfile.write(str(item))\n",
    "            if idx < len(result_list) - 1:\n",
    "                outfile.write(\",\\n\")\n",
    "            else:\n",
    "                outfile.write(\"\\n\")\n",
    "\n",
    "    print(f\"Text data written to {file_path}\")\n",
    "\n",
    "# Example usage\n",
    "dfs_to_text(dfs, \"dfs.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Keywords Matching Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find and rename column names based on possible matches\n",
    "def find_column_names(df, match_options):\n",
    "    found_columns = {}\n",
    "    for col in df.columns:\n",
    "        if col is not None:  # Ensure column names are treated case-insensitively\n",
    "            col_lower = col.lower()\n",
    "            for key, options in match_options.items():\n",
    "                if any(opt in col_lower for opt in options):\n",
    "                    if key not in found_columns:  # Prevent duplicate keys\n",
    "                        found_columns[key] = col\n",
    "                        break\n",
    "    return found_columns\n",
    "\n",
    "\n",
    "# Standard columns all DataFrames should adhere to\n",
    "standard_columns = ['Batch Number', 'Ingredient', 'Weight (%)', 'Weight(g)']\n",
    "\n",
    "processed_dfs = []\n",
    "for df in dfs:\n",
    "    # Remove duplicate columns\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    match_options = {\n",
    "        'Batch Number': ['batch number'],\n",
    "        'Ingredient': ['ingredient', 'ingrediant', 'name', 'Trade Name'],\n",
    "        'Weight (%)': ['%', 'wt. percent', 'percent', '% w/w', '% wiw'],\n",
    "        'Weight(g)': ['Batch Quantity', 'g', 'Batch (g)', 'Batch(g)', 'Weight(g)']\n",
    "    }\n",
    "\n",
    "    # Find actual columns based on match options\n",
    "    actual_columns = find_column_names(df, match_options)\n",
    "\n",
    "    # If matches are found, rename and reorder based on matches\n",
    "    if actual_columns:\n",
    "        df = df[list(actual_columns.values())].rename(columns={v: k for k, v in actual_columns.items()})\n",
    "    else:\n",
    "        # If no matches found, ensure the DataFrame still has the standard columns (all blank)\n",
    "        df = pd.DataFrame(columns=standard_columns)\n",
    "    \n",
    "    # Ensure the DataFrame contains all standard columns, fill missing ones with None\n",
    "    for col in standard_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None  # Add missing columns as None\n",
    "    \n",
    "    # Reorder columns to match the standard_columns order\n",
    "    df = df[standard_columns]\n",
    "    processed_dfs.append(df)\n",
    "\n",
    "# Concatenate all processed DataFrames into a single final DataFrame\n",
    "final_df = pd.concat(processed_dfs, ignore_index=True) if processed_dfs else pd.DataFrame(columns=standard_columns)\n",
    "\n",
    "print(final_df) # including weight (g) as the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the last col (weight (g)) - BMS doesn't need it in the end\n",
    "df_3cols = final_df.iloc[:, :-1]\n",
    "df_3cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-prosseing with 'Weight %' column\n",
    "# Assuming df_3cols is the dataframe you're working with\n",
    "\n",
    "# Function to clean weight % values in the dataframe： \n",
    "# Since there are some percentage signs, I want to standardize the format.\n",
    "def clean_weight(value):\n",
    "    if pd.isna(value):\n",
    "        return None  # If the value is NaN, return None\n",
    "    if isinstance(value, str):  # Ensure the value is a string\n",
    "        # Remove '%' and ',' characters\n",
    "        value = value.replace('%', '').replace(',', '')\n",
    "        # Keep only digits and decimal points\n",
    "        value = ''.join(c for c in value if c.isdigit() or c == '.')\n",
    "        try:\n",
    "            return float(value)  # Convert to float\n",
    "        except ValueError:\n",
    "            return None  # In case the value cannot be converted to float, return None\n",
    "    elif isinstance(value, (int, float)):\n",
    "        return float(value)  # If it's already a numeric type, just convert to float\n",
    "    else:\n",
    "        return None  # If it's neither, return None\n",
    "\n",
    "# Assuming df_3cols is your dataframe\n",
    "df_3cols['Weight (%)'] = df_3cols['Weight (%)'].apply(clean_weight)\n",
    "\n",
    "# Your dataframe is now cleaned: extracted the specific 3 columns successfully\n",
    "print(df_3cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final DataFrame to CSV for accuracy part\n",
    "df_3cols.to_csv('keywords_matching_dfs.csv', index=False) \n",
    "print(f\"Saved {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8a. Binary 0/1 Accuracy Metric for Keyword Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_3cols\n",
    "\n",
    "# Convert 'Weight (%)' to string and remove rows where it contains '100' or '100%'\n",
    "df = df[~df['Weight (%)'].astype(str).str.contains('^100(\\.0+)?%?$', na=False)]\n",
    "\n",
    "# Ensure that 'Weight (%)' is a string to replace '%' and convert to numeric\n",
    "df['Weight (%)'] = df['Weight (%)'].astype(str).str.replace('%', '')\n",
    "df['Weight (%)'] = pd.to_numeric(df['Weight (%)'], errors='coerce')\n",
    "\n",
    "# Group by 'Batch Number' and sum 'Weight (%)', skipping NaN values automatically\n",
    "sums = df.groupby('Batch Number')['Weight (%)'].sum()\n",
    "\n",
    "# Create a dictionary to store the batch number and accuracy\n",
    "batch_accuracy = {}\n",
    "\n",
    "# Check sums and assign accuracy, with special handling for sums greater than 1\n",
    "for batch_number, total_weight in sums.items():\n",
    "    accuracy = 1 if total_weight == 100 else 0  # assuming the sum should be 100 for 100%\n",
    "    batch_accuracy[batch_number] = accuracy\n",
    "    if total_weight > 100:\n",
    "        print(f\"Sum greater than 100% for batch {batch_number} may be because of skipped dot '.' issue. [For example: 4.5 --> 45]\")\n",
    "\n",
    "# Create a new DataFrame from the dictionary\n",
    "accuracy_df = pd.DataFrame(list(batch_accuracy.items()), columns=['Batch Number', 'Accuracy'])\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy percentage\n",
    "accuracy_percentage = (accuracy_df['Accuracy'].sum() / len(accuracy_df)) * 100\n",
    "\n",
    "print(f\"Accuracy Percentage: {accuracy_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy percentage\n",
    "accuracy_percentage = (accuracy_df['Accuracy'].sum() / len(accuracy_df)) * 100\n",
    "\n",
    "print(f\"Accuracy Percentage: {accuracy_percentage}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Post-processing for Keyword Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'file_path.csv' with the path to your keywords matching CSV file\n",
    "final_keywords_method_path = 'keywords_matching_dfs.csv'\n",
    "df = pd.read_csv(final_keywords_method_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert 'Weight (%)' from string to numeric\n",
    "df['Weight (%)'] = pd.to_numeric(df['Weight (%)'], errors='coerce')\n",
    "\n",
    "# Remove rows where 'Weight (%)' is greater than or equal to 97\n",
    "df = df[df['Weight (%)'] < 97] # because there are some total intra-granular rows (97,98.5...)\n",
    "\n",
    "# Ensure that 'Weight (%)' is a string to replace '%' and convert to numeric\n",
    "df['Weight (%)'] = df['Weight (%)'].astype(str).str.replace('%', '')\n",
    "df['Weight (%)'] = pd.to_numeric(df['Weight (%)'], errors='coerce')\n",
    "\n",
    "# Group by 'Batch Number' and sum 'Weight (%)', skipping NaN values automatically\n",
    "sums = df.groupby('Batch Number')['Weight (%)'].sum()\n",
    "\n",
    "# Create a dictionary to store the batch number and accuracy\n",
    "batch_accuracy = {}\n",
    "\n",
    "# Check sums and assign accuracy, with special handling for sums greater than 1\n",
    "for batch_number, total_weight in sums.items():\n",
    "    accuracy = 1 if total_weight == 100 or total_weight == 97 or abs(total_weight - 100) <= 0.03  else 0  # assuming the sum should be 100 for 100%, but there are three batch number have weight % sum as 97, 100.02.100.0224\n",
    "    batch_accuracy[batch_number] = accuracy\n",
    "    if total_weight > 100.02:\n",
    "        print(f\"Sum greater than 100.02% for batch {batch_number} may be because of skipped dot '.' issue or redacted ingredients.\")# [For example: 4.5 --> 45 or duplicated rows])\n",
    "\n",
    "# Create a new DataFrame from the dictionary\n",
    "accuracy_df = pd.DataFrame(list(batch_accuracy.items()), columns=['Batch Number', 'Accuracy'])\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strict 0 or 1 Accuracy for Keyword Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy percentage\n",
    "accuracy_percentage = (accuracy_df['Accuracy'].sum() / len(accuracy_df)) * 100\n",
    "\n",
    "print(f\"Accuracy Percentage for Final Data Frame.csv: {accuracy_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8b. Partial Accuracy for Keyword Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'file_path.csv' with the path to your CSV file\n",
    "final_keywords_method_path = 'keywords_matching_dfs.csv'\n",
    "df = pd.read_csv(final_keywords_method_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Weight (%)' from string to numeric\n",
    "df['Weight (%)'] = pd.to_numeric(df['Weight (%)'], errors='coerce')\n",
    "\n",
    "# Remove rows where 'Weight (%)' is greater than or equal to 94\n",
    "df = df[df['Weight (%)'] < 94]  # Removed values >= 94\n",
    "\n",
    "# Group by 'Batch Number' and sum 'Weight (%)', skipping NaN values automatically\n",
    "sums = df.groupby('Batch Number')['Weight (%)'].sum()\n",
    "\n",
    "# Create a dictionary to store the batch number and accuracy\n",
    "batch_accuracy = {}\n",
    "\n",
    "# Check sums and assign accuracy, with special handling for accuracy calculation\n",
    "for batch_number, total_weight in sums.items():\n",
    "    if total_weight == 100 or total_weight == 97 or abs(total_weight - 100) <= 0.03:\n",
    "        accuracy = 1\n",
    "    else:\n",
    "        # Partial accuracy calculation for cases where the total weight is not close to 100\n",
    "        accuracy = total_weight / 100\n",
    "\n",
    "    # Store accuracy\n",
    "    batch_accuracy[batch_number] = accuracy\n",
    "\n",
    "# Create a new DataFrame from the dictionary\n",
    "accuracy_df = pd.DataFrame(list(batch_accuracy.items()), columns=['Batch Number', 'Accuracy'])\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy percentage \n",
    "accuracy_percentage = ((accuracy_df['Accuracy'].sum()) / len(accuracy_df)) * 100\n",
    "\n",
    "print(f\"Accuracy Percentage for Final Data Frame.csv: {accuracy_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. spaCy Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in txt file containing data\n",
    "with open('dfs_txt.txt', 'r') as file:\n",
    "    # Read the string from the file\n",
    "    string_data = file.read()\n",
    "\n",
    "# Use ast.literal_eval to convert the string to a Python object\n",
    "data = ast.literal_eval(string_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"/path_to_model\")\n",
    "\n",
    "test_data = data \n",
    "\n",
    "# Initialize an empty list to store entity dictionaries\n",
    "entity_data_list = []\n",
    "\n",
    "# Process each text in the test data\n",
    "for item in test_data:\n",
    "    text = item[\"text\"]\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Initialize a dictionary to store entities for this text\n",
    "    entity_data = defaultdict(str)\n",
    "\n",
    "    # Process each entity in the text\n",
    "    for ent in doc.ents:\n",
    "        # Use entity labels as keys in the dictionary\n",
    "        label = ent.label_\n",
    "        # Exclude \"ner\" label\n",
    "        if label != \"ner\":\n",
    "            # Add entity text to the corresponding key\n",
    "            entity_data[label] += ent.text + \" \"\n",
    "\n",
    "    # Append the entity data dictionary to the list\n",
    "    entity_data_list.append(entity_data)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df = pd.DataFrame(entity_data_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('ner_results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. spaCy post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in csv and rename column\n",
    "df = pd.read_csv(\"ner_results.csv\")\n",
    "df = df.rename(columns={\"PERCENT QUANTITY\":\"Weight (%)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the weight column for rows that have repeated weights that are the same, if they are different, just keep since it is incorrect results\n",
    "def process_weight(x):\n",
    "    #split string\n",
    "    parts = str(x).split(\" \")\n",
    "    #if there are multiple elements and first and second are the same, just keep the first weight\n",
    "    if len(parts) > 1 and parts[0] == parts[1]:\n",
    "        return parts[0]\n",
    "    #if they are different, keep both since it is incorrect\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "df['Weight (%)'] = df['Weight (%)'].apply(process_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tidy up nans\n",
    "df[\"Weight (%)\"] = df[\"Weight (%)\"].replace('nan', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop column\n",
    "df = df.drop(columns = [\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove repeated ingredient names in a row (sometimes they are duplicated)\n",
    "def remove_repeats(value):\n",
    "    # Check if the value is a string\n",
    "    if isinstance(value, str):\n",
    "        # Use regex to find repeated substrings and replace them with a single instance\n",
    "        return re.sub(r'(.+?)\\1+', r'\\1', value)\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "# Apply the function to the 'Column' and create a new cleaned column\n",
    "df['INGREDIENT'] = df['INGREDIENT'].apply(remove_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometimes, the weight % ends up mislabelled as an ingredient, post process\n",
    "def move_number(row):\n",
    "    weight = row['Weight (%)']\n",
    "    ingredient = row['INGREDIENT']\n",
    "    \n",
    "    # Check if Weight (%) is NaN and INGREDIENT contains a number after the string\n",
    "    if pd.isna(weight) and isinstance(ingredient, str):\n",
    "        match = re.search(r'\\d+(\\.\\d+)?', ingredient)  # Search for number in INGREDIENT\n",
    "        if match:\n",
    "            row['Weight (%)'] = float(match.group())  # Convert the matched number to float\n",
    "            row['INGREDIENT'] = re.sub(r'\\d+(\\.\\d+)?', '', ingredient).strip()  # Remove number from INGREDIENT\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row\n",
    "df = df.apply(move_number, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any leading whitespace incase, and remove any percentages in drug loading column\n",
    "df['BATCH NUMBER'] = df['BATCH NUMBER'].str.strip()\n",
    "df['DRUG LOADING'] = df['DRUG LOADING'].str.replace('%', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for some batch records with inconsistent table formats, some rows might duplicate, so remove accordingly\n",
    "df.drop_duplicates(subset=['BATCH NUMBER', 'Weight (%)', 'INGREDIENT', 'DRUG LOADING'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10a. Binary Accuracy for spaCy output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weight (%)'] = pd.to_numeric(df['Weight (%)'], errors='coerce')\n",
    "\n",
    "# Remove rows where 'Weight (%)' is greater than or equal to 97\n",
    "df = df[df['Weight (%)'] < 97]\n",
    "\n",
    "# Ensure that 'Weight (%)' is a string to replace '%' and convert to numeric\n",
    "df['Weight (%)'] = df['Weight (%)'].astype(str).str.replace('%', '')\n",
    "df['Weight (%)'] = pd.to_numeric(df['Weight (%)'], errors='coerce')\n",
    "\n",
    "# Group by 'Batch Number' and sum 'Weight (%)', skipping NaN values automatically\n",
    "sums = df.groupby('BATCH NUMBER')['Weight (%)'].sum()\n",
    "\n",
    "# Create a dictionary to store the batch number and accuracy\n",
    "batch_accuracy = {}\n",
    "\n",
    "# Check sums and assign accuracy, with special handling for sums greater than 1\n",
    "for batch_number, total_weight in sums.items():\n",
    "    accuracy = 1 if total_weight == 100 or total_weight == 97 or abs(total_weight - 100) <= 0.02  else 0  # assuming the sum should be 100 for 100%, but there are two batch number have weight % sum as 97, 100.02\n",
    "    batch_accuracy[batch_number] = accuracy\n",
    "    if total_weight > 100.02:\n",
    "        print(f\"Sum greater than 100.02% for batch {batch_number} may be because of skipped dot '.' issue. [For example: 4.5 --> 45]\")\n",
    "\n",
    "# Create a new DataFrame from the dictionary\n",
    "accuracy_df = pd.DataFrame(list(batch_accuracy.items()), columns=['BATCH NUMBER', 'Accuracy'])\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(accuracy_df)\n",
    "\n",
    "accuracy_percentage = ((accuracy_df['Accuracy'].sum()) / len(accuracy_df)) * 100\n",
    "\n",
    "print(f\"Accuracy Percentage for Final Data Frame.csv: {accuracy_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10b. Partial Accuracy for spaCy output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weight (%)'] = pd.to_numeric(df['Weight (%)'], errors='coerce')\n",
    "\n",
    "# Remove rows where 'Weight (%)' is greater than or equal to 97\n",
    "df = df[df['Weight (%)'] < 97]  # Removed values >= 97\n",
    "\n",
    "# Group by 'Batch Number' and sum 'Weight (%)', skipping NaN values automatically\n",
    "sums = df.groupby('BATCH NUMBER')['Weight (%)'].sum()\n",
    "\n",
    "# Create a dictionary to store the batch number and accuracy\n",
    "batch_accuracy = {}\n",
    "\n",
    "# Check sums and assign accuracy, with special handling for accuracy calculation\n",
    "for batch_number, total_weight in sums.items():\n",
    "    if total_weight == 100 or abs(total_weight - 100) <= 0.03:\n",
    "        accuracy = 1\n",
    "    else:\n",
    "        # Partial accuracy calculation for cases where the total weight is not close to 100\n",
    "        accuracy = total_weight / 100\n",
    "\n",
    "    # Store accuracy\n",
    "    batch_accuracy[batch_number] = accuracy\n",
    "\n",
    "# Create a new DataFrame from the dictionary\n",
    "accuracy_df = pd.DataFrame(list(batch_accuracy.items()), columns=['BATCH NUMBER', 'Accuracy'])\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(accuracy_df)\n",
    "\n",
    "accuracy_percentage = ((accuracy_df['Accuracy'].sum()) / len(accuracy_df)) * 100\n",
    "\n",
    "print(f\"Accuracy Percentage for Final Data Frame.csv: {accuracy_percentage}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
